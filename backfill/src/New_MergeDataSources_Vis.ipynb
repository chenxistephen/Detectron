{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "from PIL import Image\n",
    "import pickle as pkl\n",
    "import cv2\n",
    "\n",
    "from detectron.datasets.json_dataset import JsonDataset\n",
    "from detectron.utils.io import load_object, save_object\n",
    "import detectron.utils.vis as vis_utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def checkMkdir(dirname):\n",
    "    if not osp.isdir(dirname):\n",
    "        os.makedirs(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = '' # {'philly', 'aml', ''}\n",
    "\n",
    "if platform == 'philly':\n",
    "    imgPathDict =  {'COCO_trainval': None,\n",
    "                    'coco_2014_train': 'coco/train2014/',\n",
    "                    'coco_2014_valminusminival': 'coco/val2014/',\n",
    "                    'furniture_train': 'HomeFurniture/Images.zip@/Images/',\n",
    "                    'FashionV2_train': 'FashionV2/Images.zip@/Images/',\n",
    "                    'OpenImage_train': 'OpenImage/train_images.zip@/train_images/',\n",
    "                    'Object365_train': 'Object365/train.zip@/train/'\n",
    "                    }\n",
    "else:\n",
    "    imgPathDict =  {'COCO_trainval': None,\n",
    "                    'coco_2014_train': 'coco/train2014/',\n",
    "                    'coco_2014_valminusminival': 'coco/val2014/',\n",
    "                    'furniture_train': 'HomeFurniture/Images/',\n",
    "                    'FashionV2_train': 'FashionV2/Images/',\n",
    "                    'OpenImage_train': 'OpenImage/train_images/',\n",
    "                    'Object365_train': 'Object365/train/'\n",
    "                    }\n",
    "        \n",
    "sampleSizeMap = {'COCO_trainval': None,\n",
    "                 'furniture_train': None,\n",
    "                 'FashionV2_train': None,\n",
    "                 'Object365_train': None,\n",
    "                 'OpenImage_train': 800000\n",
    "                }\n",
    "        \n",
    "boxFilePath = '/media/data/chnxi/GOD/threshold_bboxes/'\n",
    "mapping_date_ver = '20191017' #'201909'\n",
    "mapFilePath = '/media/data/chnxi/GOD/mappings/{}/backfill_src_nameid_to_GOD_id_mappings/'.format(mapping_date_ver)\n",
    "labelFile = '/media/data/chnxi/GOD/taxonomy/GOD_taxonomy_{}.tsv'.format(mapping_date_ver) # 'GOD_taxonomy_V1.tsv'\n",
    "\n",
    "\n",
    "modelSetMap = {'Fashion_Detector'  : 'fashion',\n",
    "               'HF_Detector'       : 'furniture',\n",
    "               'COCO_Mask_Detector': 'COCO',\n",
    "               'OpenImage_Detector': 'Open Image'}\n",
    "\n",
    "datasetList = ['Object365_train', 'furniture_train', 'COCO_trainval', 'FashionV2_train', 'OpenImage_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(god_classes) original = 846\n"
     ]
    }
   ],
   "source": [
    "#############   Creating taxonomy   #############################################\n",
    "import pandas as pd\n",
    "god_data = pd.read_csv(labelFile, delimiter='\\t')\n",
    "god_name_id_map = dict(zip(god_data['GOD_v1_name'], god_data['GOD_v1_id']))\n",
    "god_classes = list(god_data['GOD_v1_name'])\n",
    "print (\"len(god_classes) original = {}\".format(len(god_classes)))\n",
    "god_classes = ['__background__'] + god_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mergedDataset = GOD_Open800k_O365_20191017\n",
      "outAnnoFile = /media/data/chnxi/GOD/json_annotations/GOD_Open800k_O365_20191017_train.json\n"
     ]
    }
   ],
   "source": [
    "#############   Creating Data   ################################################# \n",
    "#####  Creating one json for 4 datasets\n",
    "platformStr = '_' + platform if len(platform) > 0 else ''\n",
    "openStr = 'Open{}k'.format(int(sampleSizeMap['OpenImage_train']/1000)) if sampleSizeMap['OpenImage_train'] is not None else 'All'\n",
    "mergedDataset = 'GOD_{}_O365_{}{}'.format(openStr, mapping_date_ver, platformStr) #'GOD_Open40k'\n",
    "# totalSampleNum = 0\n",
    "# for dataset in datasetList:\n",
    "#     if sampleSizeMap[dataset] is not None:\n",
    "#         totalSampleNum += sampleSizeMap[dataset]\n",
    "# mergedDataset = '{}_{}'.format(mergedDataset, totalSampleNum) if totalSampleNum is not None else mergedDataset\n",
    "print (\"mergedDataset = {}\".format(mergedDataset))\n",
    "outAnnoFolder = '/media/data/chnxi/GOD/json_annotations/'\n",
    "checkMkdir(outAnnoFolder)\n",
    "outAnnoFile = osp.join(outAnnoFolder, '{}_train.json'.format(mergedDataset))\n",
    "print (\"outAnnoFile = {}\".format(outAnnoFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "supercat = 'Generic'\n",
    "data['info'] = {'description':'Bing Generic Object Detection dataset on {}'.format(mergedDataset)}\n",
    "data['licenses'] = [{}]\n",
    "data['type'] = 'instances'\n",
    "\n",
    "data['categories'] = []\n",
    "\n",
    "for cls, id in god_name_id_map.items():\n",
    "    cats = {'supercategory':supercat,\n",
    "            'id':id,\n",
    "            'name':cls}\n",
    "    data['categories'].append(cats)\n",
    "\n",
    "totalBoxCnt = 0\n",
    "totalImgCnt = -1\n",
    "data['images'] = []\n",
    "data['annotations'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "def getAnno(image_id, bbox, categ_id, box_id, anno_src='gt'):\n",
    "    image_id = int(image_id) # convert np.ndarray.int64 to int in case json.dump not serializable\n",
    "    boxw = float(bbox[2]-bbox[0]) # convert np.ndarray.float32 to float in case json.dump not serializable\n",
    "    boxh = float(bbox[3]-bbox[1])\n",
    "    nbbox = [float(bbox[0]), float(bbox[1]), boxw, boxh]\n",
    "    anno = {'segmentation':[],\n",
    "            'area': boxw*boxh,\n",
    "            'iscrowd':0,\n",
    "            'image_id':image_id,\n",
    "            'bbox':nbbox, # because COCO format [l, t, w,h] #float(r),float(b)],\n",
    "            'category_id':categ_id,\n",
    "            'id':box_id,\n",
    "            'anno_src': anno_src\n",
    "            }\n",
    "    return anno\n",
    "#################################################################################\n",
    "def filterAnnoCondition(dataset, modelName, god_class_name=None):\n",
    "    if dataset == 'furniture_train' and modelName in ['COCO_Mask_Detector', 'OpenImage_Detector']:\n",
    "        if 'home_or_office_furnishing_or_decor' not in god_class_name:\n",
    "            #print (\"filtering {} from {} in {}\".format(god_class_name, modelName, dataset))\n",
    "            return True\n",
    "    return False\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Object365_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize = sampleSizeMap[dataset]\n",
    "print (\"sampleSize = {}\".format(sampleSize))\n",
    "imgPath = imgPathDict[dataset]\n",
    "print (\"imgPath = {}\".format(imgPath))\n",
    "if dataset == 'furniture_train':\n",
    "    gtSetName = 'furniture'\n",
    "    # Only backfills \"dinnerware_serveware\" from OpenImage/COCO;  No fashion backfills on HF\n",
    "    mapFile = mapFilePath + 'HF_dataset_sources_ids.pkl'\n",
    "    srcFiles = {#'Fashion_Detector'  : 'furniture_train.json',\n",
    "                'COCO_Mask_Detector': 'furniture_train.pkl',\n",
    "                'OpenImage_Detector': 'furniture_train.pkl'}\n",
    "elif dataset == 'FashionV2_train':\n",
    "    gtSetName = 'fashion'\n",
    "    #imgPath = 'FashionV2/Images/'\n",
    "    mapFile = mapFilePath + 'Fashion_dataset_sources_ids.pkl'\n",
    "    srcFiles = {'HF_Detector'       : 'FashionV2_train.pkl',\n",
    "                'COCO_Mask_Detector': 'FashionV2_train.pkl',\n",
    "                'OpenImage_Detector': 'FashionV2_train.pkl'}\n",
    "elif dataset == 'Object365_train':\n",
    "    gtSetName = 'O365'\n",
    "    #imgPath = 'OpenImage/train_images/'\n",
    "    mapFile = mapFilePath + 'Object365_dataset_sources_ids.pkl'\n",
    "    srcFiles = {}\n",
    "elif dataset == 'OpenImage_train':\n",
    "    gtSetName = 'Open Image'\n",
    "    #imgPath = 'OpenImage/train_images/'\n",
    "    mapFile = mapFilePath + 'OpenImage_dataset_sources_ids.pkl'\n",
    "    srcFiles = {'Fashion_Detector'  : 'OpenImage_train.json',\n",
    "                'HF_Detector'       : 'OpenImage_train.pkl',\n",
    "                'COCO_Mask_Detector': 'OpenImage_train.pkl'}\n",
    "elif dataset == 'COCO_trainval':\n",
    "    gtSetName = 'COCO'\n",
    "    mapFile = mapFilePath + 'COCO_dataset_sources_ids.pkl'\n",
    "    srcFiles = {'Fashion_Detector'  : 'COCO_trainval.json',\n",
    "                'HF_Detector'       : 'coco_2014_trainval.pkl',\n",
    "                'OpenImage_Detector': 'COCO_trainval.pkl'}\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "### Loading ROIDB   #############################################################\n",
    "if dataset == 'COCO_trainval':\n",
    "    datasetNames = ('coco_2014_train','coco_2014_valminusminival')\n",
    "    roidb = []\n",
    "    for dsName in datasetNames:\n",
    "        ds = JsonDataset(dsName)\n",
    "        rdb = ds.get_roidb(gt=True)\n",
    "        roidb = roidb + rdb\n",
    "else: #if dataset in ['furniture_train', 'FashionV2_train', 'OpenImage_train', 'Object365_train']:\n",
    "    print (\"creating json dataset {}\".format(dataset))\n",
    "    ds = JsonDataset(dataset)\n",
    "    roidb = ds.get_roidb(gt=True)\n",
    "imgNum = len(roidb)\n",
    "print ('imgNum = {}'.format(imgNum))\n",
    "if sampleSize is not None:\n",
    "    sampleSize = min(imgNum, sampleSize) \n",
    "dbSampleNum = sampleSize if sampleSize is not None else imgNum\n",
    "print (\"sampling {} images from roidb {}\".format(dbSampleNum, dataset))\n",
    "#################################################################################\n",
    "\n",
    "### Loading ROIDB   #############################################################\n",
    "print (\"loading {}\".format(mapFile))\n",
    "src_god_map = pkl.load(open(mapFile,'rb'))\n",
    "print (src_god_map.keys())\n",
    "#     cocoMap = src_god_map['name_to_god_id']['COCO']\n",
    "#     print (cocoMap)\n",
    "#     cocoIdMap = src_god_map['id_map']['COCO']\n",
    "#     print(cocoIdMap)\n",
    "#     print (len(cocoMap))\n",
    "#################################################################################\n",
    "\n",
    "### Loading model bboxes   ######################################################              \n",
    "print (\"Loading model bboxes!\")\n",
    "srcBoxes = {}\n",
    "for modelName in srcFiles:\n",
    "    box_file_name = srcFiles[modelName]\n",
    "    boxFileName = osp.join(boxFilePath, modelName, box_file_name)\n",
    "    print (\"Loading boxFileName = {}\".format(boxFileName))\n",
    "    if modelName in ['COCO_Mask_Detector', 'HF_Detector']:\n",
    "        # format: all_boxes[totalImgCnt][class_id]\n",
    "        all_boxes = load_object(boxFileName)\n",
    "        print (\"loaded\")\n",
    "        print (\"all_boxes.size = ({}, {})\".format(len(all_boxes), len(all_boxes[0])))\n",
    "        srcBoxes[modelName] = all_boxes\n",
    "    elif modelName == 'OpenImage_Detector':\n",
    "        # all_boxes[file_name] = {'file_name': file_name,\n",
    "        #        'boxes': boxes,\n",
    "        #        'label_names': label_names,\n",
    "        #        'labels': labels}\n",
    "        all_boxes = load_object(boxFileName)\n",
    "        print (\"len(all_boxes) = {}\".format(len(all_boxes)))\n",
    "        srcBoxes[modelName] = all_boxes\n",
    "    elif '.json' in box_file_name:\n",
    "        # {'image_id': 'd7a0b91d2d83d457', \n",
    "        #'boxes': [{'score': 0.228, 'category_id': 2, \n",
    "        #'bbox': [506.49920000000003, 414.4128, 657.8248, 544.1536], \n",
    "        #'category_name': 'accessories-glasses'}], 'image_file_name': 'd7a0b91d2d83d457.jpg'}\n",
    "        print (\"loading {}\".format(boxFileName))\n",
    "        dets = json.load(open(boxFileName,'r'))\n",
    "        all_boxes = {}\n",
    "        for ix, det in enumerate(dets):\n",
    "            # make it a dictionary with image file name as key\n",
    "            img_file_name = det['image_file_name']\n",
    "            all_boxes[img_file_name] = det['boxes']\n",
    "        print (\"len(all_boxes) = {}\".format(len(all_boxes)))\n",
    "        print (\"loaded\")\n",
    "        srcBoxes[modelName] = all_boxes #dets\n",
    "    print (\"srcBoxes[{}] loaded\".format(modelName))\n",
    "###################### Sampling Image index  ################################################\n",
    "if sampleSize is not None:\n",
    "    sampleInds = np.random.choice(imgNum, sampleSize)\n",
    "else:\n",
    "    sampleInds = list(range(imgNum))\n",
    "\n",
    "print (\"len(sampleInds) = {} ==========\".format(len(sampleInds)))\n",
    "\n",
    "########### Getting bbox from 4 sources  ################################################\n",
    "\n",
    "for iix, imgid_in_dataset in enumerate(sampleInds):\n",
    "    imgid_in_dataset = int(imgid_in_dataset)\n",
    "    entry = roidb[imgid_in_dataset]\n",
    "    totalImgCnt += 1 # start from -1 + 1 = 0\n",
    "    ###############################################\n",
    "    ## data['images']\n",
    "    ###############################################\n",
    "    file_name = entry['image']\n",
    "    imgName = osp.basename(file_name)\n",
    "    if dataset == 'COCO_trainval':\n",
    "        if 'train' in imgName:\n",
    "            imgPath = imgPathDict['coco_2014_train'] #'coco/train2014/'\n",
    "        elif 'val' in imgName:\n",
    "            imgPath = imgPathDict['coco_2014_valminusminival'] #'coco/val2014/'\n",
    "    imgFileName = imgPath + imgName\n",
    "    imgH = entry['height']\n",
    "    imgW = entry['width']\n",
    "    img = {'file_name': imgFileName,\n",
    "           'height': entry['height'],\n",
    "           'width': entry['width'],\n",
    "           'id':totalImgCnt # bug: imgid\n",
    "           }\n",
    "    data['images'].append(img)\n",
    "\n",
    "    #print (img)\n",
    "    ###############################################\n",
    "    ## Add GT Boxes\n",
    "    ###############################################\n",
    "    src_god_id_map = src_god_map['id_map'][gtSetName]\n",
    "    src_name_godid_map = src_god_map['name_to_god_id'][gtSetName]\n",
    "    visName = gtSetName + '_GT'\n",
    "    anno_src = gtSetName + '_GT'\n",
    "    gt_boxes = entry['boxes']\n",
    "    gt_src_cls_ids = entry['gt_classes']\n",
    "    for bid, box in enumerate(gt_boxes):\n",
    "        src_class_id = gt_src_cls_ids[bid]\n",
    "        if src_class_id in src_god_id_map:\n",
    "            god_class_id = src_god_id_map[src_class_id]\n",
    "            #print (\"======{}:{} ==> GOD:{} {}\".format(gtSetName, src_class_id, god_class_id, god_classes[god_class_id]))\n",
    "            anno = getAnno(totalImgCnt, box[:4], god_class_id, totalBoxCnt, anno_src=anno_src)\n",
    "            #print (anno)\n",
    "            data['annotations'].append(anno)\n",
    "            totalBoxCnt += 1\n",
    "    ###############################################\n",
    "    ## Add boxes from other models\n",
    "    ###############################################\n",
    "    for modelName in srcFiles:\n",
    "        modelSetName = modelSetMap[modelName]\n",
    "        src_god_id_map = src_god_map['id_map'][modelSetName]\n",
    "        src_name_godid_map = src_god_map['name_to_god_id'][modelSetName]\n",
    "        # print (\"{} ==> {}\".format(modelName, modelSetName))\n",
    "        if modelName in ['COCO_Mask_Detector', 'HF_Detector']:\n",
    "            # format: all_boxes[imgid_in_dataset][class_id]\n",
    "            for cid, cls_boxes in enumerate(srcBoxes[modelName][imgid_in_dataset]):\n",
    "                src_class_id = cid + 1 # all_boxes starts from 0\n",
    "                if src_class_id in src_god_id_map and len(cls_boxes) > 0:\n",
    "                    god_class_id = src_god_id_map[src_class_id]\n",
    "                    #print (\"========{}:{} ==> GOD:{}:{}\".format(modelSetName, src_class_id, god_class_id, god_classes[god_class_id]))\n",
    "                    for bbox in cls_boxes:\n",
    "                        anno = getAnno(totalImgCnt, bbox, god_class_id, totalBoxCnt, anno_src=modelName)\n",
    "                        if filterAnnoCondition(dataset, modelName, god_class_name=god_classes[god_class_id]):\n",
    "                            continue\n",
    "                        data['annotations'].append(anno)\n",
    "                        totalBoxCnt += 1\n",
    "        if modelName == 'OpenImage_Detector':\n",
    "            # all_boxes[file_name] = {'file_name': file_name,\n",
    "            #        'boxes': boxes,\n",
    "            #        'label_names': label_names,\n",
    "            #        'labels': labels}\n",
    "            boxes = srcBoxes[modelName][imgName]['boxes']\n",
    "            label_names = srcBoxes[modelName][imgName]['label_names']\n",
    "            for bid, box in enumerate(boxes):\n",
    "                src_class_name = label_names[bid] # do not use labels: it's not correct for OpenImage Id\n",
    "                if src_class_name in src_name_godid_map:\n",
    "                    god_class_id = src_name_godid_map[src_class_name]\n",
    "                    #print (\"========= {}:{} ==> GOD:{}:{}\".format(modelSetName, src_class_name, god_class_id, god_classes[god_class_id]))\n",
    "                    bbox = [box[0] * imgW, box[1] * imgH, box[2] * imgW, box[3] * imgH]\n",
    "                    if filterAnnoCondition(dataset, modelName, god_class_name=god_classes[god_class_id]):\n",
    "                        continue\n",
    "                    anno = getAnno(totalImgCnt, bbox, god_class_id, totalBoxCnt, anno_src=modelName)\n",
    "                    #print (anno)\n",
    "                    data['annotations'].append(anno)\n",
    "                    totalBoxCnt += 1                 \n",
    "        if modelName == 'Fashion_Detector': #'.json' in box_file_name:\n",
    "            # all_boxes[imgName] = [ {[506.49920000000003, 414.4128, 657.8248, 544.1536], \n",
    "            #'category_name': 'accessories-glasses'} ]\n",
    "            if imgName in srcBoxes[modelName]:\n",
    "                boxes = srcBoxes[modelName][imgName]\n",
    "                for bid, obj in enumerate(boxes):\n",
    "                    src_class_name = obj['category_name']\n",
    "                    if src_class_name in src_name_godid_map:\n",
    "                        #print (obj)\n",
    "                        god_class_id = src_name_godid_map[src_class_name]\n",
    "                        #print (\"========{}:{} ==> GOD:{}{}\".format(modelSetName, src_class_name, god_class_id, god_classes[god_class_id]))\n",
    "                        bbox = obj['bbox']\n",
    "                        anno = getAnno(totalImgCnt, bbox, god_class_id, totalBoxCnt, anno_src=modelName)\n",
    "                        #print (anno)\n",
    "                        data['annotations'].append(anno)\n",
    "                        totalBoxCnt += 1\n",
    "    if (iix + 1) % 100 == 0:\n",
    "        print (\"{}/{}\".format(iix + 1, len(sampleInds)))\n",
    "        print (\"img = {}\".format(img))\n",
    "        anno = data['annotations'][-1]\n",
    "        cat_id = anno['category_id']\n",
    "        cat_name = data['categories'][cat_id-1]['name']\n",
    "        print (\"{}:{}, cat_id:name = {}::{}, (x,y,w,h) = {}\".format(totalImgCnt, imgName, cat_id, cat_name, anno['bbox']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
